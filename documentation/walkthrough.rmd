---
title: "Turnout Tracker Walkthrough"
author: "Jonathan Tannen"
date: "March 11, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning=FALSE,
  message=FALSE,
  dpi=300,
  root.dir = "C:/Users/Jonathan Tannen/Dropbox/sixty_six/posts/election_day_tracker/tracker_v0/"
)

setwd("C:/Users/Jonathan Tannen/Dropbox/sixty_six/posts/turnout_tracker/tracker_v0/")
```


## Introduction
This document is a walkthrough of how to use the code for the Turnout Tracker. For a discussion of the math and the model, see [Turnout Tracker Math](methodology.html)

## Before Election Day: Fitting the Model

There are a number of parameters that need to be fit on historical data: baseline turnout rates, precinct covariances, etc. 

Each election should have a config, which I've created in config.R. `config` is a list with the following items:
```{r config}
source("util_tracker.R")
election_dir <- "elections/phila_201911"
add_election_path <- function(file) sprintf("%s/%s", election_dir, file)

library(tidyverse)

source(add_election_path("config.R"))
print(config)
```

The helper function `prep_shapefile` will load the shapefiles, process them, and then save `sf` objects `precincts.Rds` and ``wards.Rds` in `data`.

```{r prep_shapefiles}
source("prep_shapefiles.R")
prep_shapefile(
  add_election_path(config$precinct_shp_path),
  config$get_precinct_id,
  config$get_ward_from_precinct,
  save_dir = add_election_path("data")
)
```

Before election day, we need to calculate the historic fixed effects and correlations. All of the prep work is done in `calc_params`. The input is a dataframe, `turnout_df`, which has columns `precinct, year, turnout`. Precinct is the unique identifier for the precinct, year is the year, and turnout is the voter count. You will need to [crosswalk](https://sixtysixwards.com/home/how-i-crosswalk-election-data turnout) to the present-day precincts if boundaries have changed.

```{r turnout_df}
df <- read_csv(add_election_path(config$turnout_df_path), col_types = "ccd")
head(arrange(df, precinct, election))
```

We can now calculate the historic `modelParams`:
```{r calc_params}
source("precalc_params.R")

params <- calc_params(
  turnout_df=df, 
  n_svd=3
) 

## params has a copy of turnout_df, with some new columns.
print(head(params@turnout_df))

## params has an estimate of the election_fe, on the log scale.
print(head(params@election_fe))

## params has an estimate of the precinct_fe, on the log scale.
print(head(params@precinct_fe))

## params has the svd results, which is used for the covariance (more on this later).
print(head(params@svd$u))
print(head(params@svd$v))

## params has the estimated covariance matrix among precincts (and its inverse)
print(params@precinct_cov[1:6, 1:6])
```

I also provide some helper functions to make diagnostic plots. These require an `sf` object with the precinct shapefiles. (The outputs of `prep_shapefile` suffice)

The diagnostics include plots of (a) the fixed effects by precinct and by year, and (b) the svd components for the estimated covariances, along with each dimension's score in each year. You should sanity check that the combination of precincts and elections make sense.
```{r diagnostics}
library(sf)
## Finally, check out some diagnostic plots...
divs <- readRDS(add_election_path("data/precincts.Rds"))
diagnostics(params, divs, config, pause=FALSE)
```

The plots look good. Dimension 1 is blue for Hispanic North Philly and the University of Pennsylvania, and the line plot shows that these precincts had disproportionately high turnout in 2004, 2008, 2012, 2016 (the presidential elections). Dimension 2 has captured population change (red divisions are increasing, blue divisions are decreasing). Dimension 3 is hard to interpret, and may be noise/misfitting...

Let's save the results and move on.
```{r save_params}
save_with_backup(params, stem="params", dir=add_election_path("outputs"))
```

## Testing on Fake Data
An important validation is to test the model on a fake, known distribution. The function `load_data` will either load data from our google-form download (later), or create a fake dataset with an S-curve.
```{r fake_data}
source("fit_submissions.R")

set.seed(215)
data_list <- load_data(use_google_data=FALSE, params=params, election_config=config)
raw_data <- data_list$raw_data
fake_data <- data_list$fake_data

em_fit <- fit_em_model(
  raw_data, params, verbose=FALSE, tol=1e-10, use_inverse=FALSE, election_config = config
)

fit <- process_results(
  model_fit=em_fit, 
  election_config=config,
  calc_ses=FALSE, 
  verbose=TRUE,
  plots = TRUE, 
  save_results = FALSE,
  fake_data=fake_data,
  pause=FALSE
)

print("Estimate:")
fit@full_predictions %>% 
  filter(time_of_day == max(time_of_day)) %>%
  with(sum(prediction))
```

But we don't want a single estimate, we want a bootstrap of estimates. This can take a few minutes, so I'll just do 40 iterations for the demo...:
```{r bootstrap}
source("bootstrap.R")

bs <- fit_and_bootstrap(
  raw_data=raw_data,
  params=params,
  election_config=config,
  n_boot=40,
  use_inverse=FALSE,
  verbose=TRUE
)

gg_bs_hist <- hist_bootstrap(bs) 
print(gg_bs_hist)

gg_turnout <- turnout_plot(
  bs,
  config
)
print(gg_turnout)
```

## On Election Day
There are a few specifics that need to be handled on election day. The file `run_all.R` does five things: (a) download the google data, (b) calculate the bootstrapped estimate, (c) compiles `election_tracker.Rmd`, which creates the markdown report, (d) pushes the html to github using `upload_git.bat`, and then (e) maybe tweets an update.

On election day, you just run `run_all.R` and let the script loop.
